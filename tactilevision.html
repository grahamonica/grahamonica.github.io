<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tactile Vision</title>
  <link rel="stylesheet" href="styles.css" />
  <link
    href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap"
    rel="stylesheet"
  />
  <style>
   body {
     background-color: #a1a2aa;
     color: #2a2a2f;           /* Dark text for legibility */
     font-family: 'Inter', sans-serif;
     margin-top: 50px;
     padding: 20px;
   }


   .container {
     max-width: 85%;
     margin: auto;
     background: #efede7; /* Soft muted blue */
     padding: 30px;
     border-radius: 12px;
     box-shadow: 0 4px 20px rgba(43, 78, 140, 0.15); /* Subtle blue-ish shadow */
   }


   .header {
     display: flex;
     align-items: center;
     gap: 20px;
     margin-bottom: 20px;
   }
   .back-link,
   .github-link {
     text-decoration: none;
     color: #2B4E8C; /* Muted darker blue */
     font-weight: 500;
     transition: color 0.3s ease;
   }
   .back-link:hover,
   .github-link:hover {
     color: #1C3B6E; /* Darker hover state */
   }


   /* Bio Section: slightly darker container for contrast */
   .bio {
     background: #d8dae0;
     padding: 20px;
     border-radius: 12px;
     margin-bottom: 20px;
     box-shadow: inset 0 0 10px rgba(0, 0, 0, 0.05);
     font-size: 1.1rem;
     line-height: 1.6;
   }


   /* README Container: same darker background as .bio for consistency */
   .readme-container {
     background-color: #d8dae0;
     padding: 20px;
     border-radius: 12px;
     margin-bottom: 20px;
     box-shadow: inset 0 0 10px rgba(0, 0, 0, 0.05);
     /* Removed overflow and max-height so the container scales with its content */
   }
   .readme-container h1,
   .readme-container h2,
   .readme-container h3 {
     color: #221f1f; /* Dark text for headings */
   }
    .screenshot-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); /* Responsive grid */
      gap: 15px; /* Space between images */
      margin-top: 20px;
    }

    .screenshot-grid img {
      width: 100%; /* Ensure images fill their grid cells */
      height: auto;
      border-radius: 8px;
    }

    .readme-container ul {
      padding-left: 20px;
    }

    .readme-container ul li {
      margin-bottom: 10px;
    }
  </style>
</head>
<body>
  <div class="container">
    <!-- Header -->
<div style="
display: flex;
justify-content: space-between;
align-items: center;
margin-bottom: 1rem;
">
<a href="index.html" class="back-link">
  Back to Portfolio
</a>
<a 
  href="https://devpost.com/software/tactilevision#updates"
  class="github-link"
  target="_blank"
>
  View Project on Devpost
</a>
</div>
<hr />

    <!-- Title -->
    <h1>Tactile Vision</h1>

    <!-- Short Bio -->
    <div class="bio">
      <p>
        Tactile Vision is a project aimed at providing mobility assistance for the visually impaired by converting sensor data into tactile feedback via a custom headset and handheld device.
      </p>
    </div>

    <!-- Project Details -->
    <div class="readme-container">
      <h2>Hackathon Winning Iteration 1</h2>
      <p>
      </p>
      <p>I led a team of four to design this:</p>
      <h3>Inspiration</h3>
      <p>
        One of our close friends has a disability that requires her to have a service dog. This experience highlighted the challenges—be it cost, allergies, or the ability to care for a service animal—that prevent many from obtaining necessary mobility assistance. With Tactile Vision, we aim to eliminate those obstacles.
      </p>
      <h3>What it does</h3>
      <p>
        Tactile Vision collects data from sensors mounted on a custom headset and translates that information into a “heatmap” on a handheld device. The headset features six evenly spaced ultrasonic sensors connected to a Raspberry Pi Pico W (nicknamed “megamind”), which communicates via Bluetooth with a second Raspberry Pi. This second unit controls the handheld device, where resistor arrays heat up to indicate both the direction and proximity of obstacles. (For example, the closest resistor indicates an obstacle 0–2 ft away, the next 2–4 ft away, and so on.) Note that while we achieved partial functionality during testing, a fully functional prototype was not realized due to PCB etching issues.
      </p>
      <h3>How we built it</h3>
      <p>
        We began by brainstorming and assessing various designs for mobility assistance. After settling on Tactile Vision, we designed a headset in SolidWorks and created a custom circuit board in KiCAD. The headset was 3D printed using flexible TPU filament, and the PCB was produced via laser and mill. Following assembly—soldering approximately 100 components—we programmed the controllers in Python to read sensor data, transmit it via Bluetooth, and drive the tactile feedback on the handheld device.
      </p>
      <h3>Challenges we ran into</h3>
      <p>
        We encountered several challenges: the PCB laser cutter struggled to cut through the copper layer, Python code bugs proved daunting, and a shortage of ultrasonic sensors complicated our build. Despite these hurdles, our team persisted—Dan resolved the PCB issues, Justin meticulously debugged the code, and Monica sourced extra sensors through community contacts.
      </p>
      <h3>Accomplishments that we're proud of</h3>
      <p>
        Team Tactile Vision emerged successful; we honed new skills, collaborated effectively, and produced a prototype we’re proud of. Each obstacle was met with determination, and our diverse backgrounds combined to make a truly innovative project.
      </p>
      <h3>What we learned</h3>
      <p>
        Throughout HackGTX, we refined our abilities in SolidWorks, KiCAD, Python, GitHub, and various fabrication techniques. Each team member contributed unique strengths, and our collaborative efforts allowed us to overcome challenges and grow professionally.
      </p>
    </div>

    <!-- Current Progress -->
    <div class="readme-container">
      <h2>Current Progress on Iteration 2</h2>
      <p>CAD model here.</p>
      <p>
        We are now using the depth API from the MetaQuest 3 to test the latest handset. The new prototype incorporates capacitive touch sensing so that resistors only activate (at safe levels) when a hand is detected—preventing dangerous overheating when the hand is removed.
      </p>
    </div>
  </div>
</body>
</html>